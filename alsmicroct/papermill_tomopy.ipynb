{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import dxchange\n",
    "from IPython import display\n",
    "from tqdm.auto import tqdm\n",
    "import nbclient.exceptions\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import papermill as pm\n",
    "\n",
    "from ipypathchooser import PathChooser\n",
    "from ipysliceviewer import SliceViewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input parameters, create directories and stage input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RTV_18A_air_760torr_08_fast\"\n",
    "task_prefix = \"papermill_tomopy_{}\"\n",
    "\n",
    "# write results to $SCRATCH\n",
    "curdir = os.getcwd()\n",
    "scratchdir = os.path.expandvars(\"$SCRATCH\")\n",
    "sessiondir = os.path.join(scratchdir, task_prefix.format(datetime.datetime.now().isoformat()))\n",
    "imagesdir = os.path.join(sessiondir, \"{}_images\".format(dataset_name))\n",
    "\n",
    "# create the output directory as needed\n",
    "if not os.path.exists(sessiondir):\n",
    "    os.mkdir(sessiondir)\n",
    "\n",
    "# create the images subdir\n",
    "if not os.path.exists(imagesdir):\n",
    "    os.mkdir(imagesdir)\n",
    "    \n",
    "# copy our input data directory to $SCRATCH for execution\n",
    "src = '/global/cfs/cdirs/als/users/parkinson/SLS_Feb2019/disk1/RTV_18A_air_760torr_08_fast'\n",
    "inputdir = os.path.join(scratchdir, \"RTV_18A_air_760torr_08_fast\")\n",
    "\n",
    "# create the input directory on $SCRATCH as needed and copy the data\n",
    "if not os.path.exists(inputdir):\n",
    "    shutil.copytree(src, inputdir)\n",
    "\n",
    "# set the parameters we want to pass to the notebook\n",
    "params = dict(\n",
    "    filename = \"RTV_18A_air_760torr_08_fast.h5\",\n",
    "    inputPath = inputdir,\n",
    "    fulloutputPath = imagesdir,\n",
    "    chunk_proj=1,\n",
    "    chunk_sino=1,\n",
    "    ncore=1,\n",
    "    filetype='sls',\n",
    "    cor=385\n",
    ")\n",
    "\n",
    "# this is the notebook we will be running through papermill, which calls the reconstruction code\n",
    "in_nb = os.path.join(curdir, 'tomopy_recon_template.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to our Dask cluster running inside of a Cori job, or a local Dask cluster if no job\n",
    "from dask.distributed import Client, as_completed\n",
    "\n",
    "localcluster = False\n",
    "if os.path.exists(\"dask_client\"):\n",
    "    try:\n",
    "        with open(\"dask_client\", 'r') as f:\n",
    "            scheduler_file = f.read().strip()\n",
    "        \n",
    "        dask_client = Client(scheduler_file=scheduler_file)\n",
    "    except Exception as e:\n",
    "        print(\"Unable to use existing dask_client file to connect to a Dask cluster!\")\n",
    "        localcluster = True\n",
    "else:\n",
    "    localcluster = True\n",
    "\n",
    "if localcluster:\n",
    "    # No Dask cluster present, we will start a local cluster\n",
    "    from dask.distributed import LocalCluster    \n",
    "    \n",
    "    cluster = LocalCluster(n_workers=4, threads_per_worker=2)\n",
    "    dask_client = Client()\n",
    "    \n",
    "dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all timepoints and collect Papermill results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all chunks\n",
    "#timepoints range from 0 to 105\n",
    "first_timepoint = 0\n",
    "last_timepoint = 10\n",
    "submits = []\n",
    "fails = []\n",
    "\n",
    "# save the input parameters out to a file with the output notebooks and data\n",
    "input_params_file = os.path.join(sessiondir, task_prefix.format(\"input_parameters.json\"))\n",
    "with open(input_params_file, 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "# submit all papermill tasks to Dask\n",
    "num_points = last_timepoint - first_timepoint\n",
    "with tqdm(total=num_points, desc=\"Tasks submitted\", unit=\"task\") as submits_pbar:\n",
    "    for timepoint in range(first_timepoint, last_timepoint):\n",
    "        params['timepoint'] = timepoint\n",
    "        out_nb = os.path.join(sessiondir, '{}.ipynb'.format(task_prefix.format(timepoint)))\n",
    "\n",
    "        submits.append(\n",
    "            dask_client.submit(\n",
    "                pm.execute_notebook, \n",
    "                in_nb, \n",
    "                out_nb, \n",
    "                params,\n",
    "                start_timeout=60,\n",
    "                progress_bar=False))\n",
    "        submits_pbar.update(1)\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"{}: {} tasks submitted, {} timepoints\".format(params['filename'], len(submits), num_points))\n",
    "\n",
    "last_exc = None\n",
    "completed = []\n",
    "failed = []\n",
    "# wait for all the tasks to complete\n",
    "with tqdm(total=len(submits), desc=\"Tasks completed\", unit=\"task\") as completed_pbar:\n",
    "    for future in as_completed(submits):\n",
    "        try:\n",
    "            x = future.result()\n",
    "            completed.append(x)\n",
    "        except nbclient.exceptions.DeadKernelError as e:\n",
    "            timepoint = submits.index(future)\n",
    "            params['timepoint'] = timepoint\n",
    "            failed.append(params)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            timepoint = submits.index(future)\n",
    "            params['timepoint'] = timepoint\n",
    "            failed.append(params)\n",
    "        finally:\n",
    "            completed_pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resubmit failed tasks, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup any remaining tasks, if present\n",
    "\n",
    "resubmits = []\n",
    "while len(failed) > 0:\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    with tqdm(total=len(failed), desc=\"Tasks resubmitted\", unit=\"task\") as resubmits_pbar:\n",
    "        for task_params in failed:\n",
    "            out_nb = os.path.join(sessiondir, '{}.ipynb'.format(task_prefix.format(task_params['timepoint'])))\n",
    "\n",
    "            resubmits.append(\n",
    "                dask_client.submit(pm.execute_notebook, in_nb, out_nb, task_params, progress_bar=False))\n",
    "            resubmits_pbar.update(1)\n",
    "            time.sleep(1)\n",
    "\n",
    "    with tqdm(total=len(resubmits), desc=\"Cleanup Tasks completed\", unit=\"task\") as cleanup_pbar:\n",
    "        for future in as_completed(resubmits):\n",
    "            try:\n",
    "                x = future.result()\n",
    "                \n",
    "                # clear the failure\n",
    "                i = resubmits.index(future)\n",
    "                failed[i] = None\n",
    "                resubmits[i] = None\n",
    "                completed.append(x)\n",
    "            except RuntimeError as e:\n",
    "                last_exc = e    \n",
    "            finally:\n",
    "                cleanup_pbar.update(1)\n",
    "    \n",
    "    failed = [f for f in failed if f is not None]\n",
    "    resubmits = [r for r in resubmits if r is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the output data directory and contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output notebooks and data in:\\n {}\\n\\n\".format(sessiondir))\n",
    "\n",
    "num_notebooks = 0\n",
    "for entry in os.scandir(sessiondir):\n",
    "    if entry.name.endswith(\".ipynb\"):\n",
    "        num_notebooks += 1\n",
    "\n",
    "num_images = 0\n",
    "last_index = 0\n",
    "for entry in os.scandir(imagesdir):\n",
    "    if entry.name.endswith(\".tiff\"):\n",
    "        num_images += 1\n",
    "\n",
    "        current_index_string = entry.name.split(\"_\")[-1].split('.')[0]\n",
    "\n",
    "        if '-' in current_index_string:\n",
    "            current_index_string = current_index_string.split('-')[0]\n",
    "\n",
    "        current_index = int(current_index_string)\n",
    "\n",
    "        if last_index < current_index:\n",
    "            last_index = current_index\n",
    "        \n",
    "print(\"{} Jupyter Notebooks were created\".format(num_notebooks))\n",
    "print(\"{} Images were created, with last_index: {}\".format(num_images, last_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a quick preview of a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preselect an image in the middle of the stack\n",
    "mid_timepoint = \"{:05d}\".format(int((last_index / 2)) + 1)\n",
    "fname = os.path.join(\n",
    "    imagesdir, \n",
    "    \"{}_{}.tiff\".format(dataset_name, mid_timepoint))\n",
    "\n",
    "# Alternatively, select an image to preview\n",
    "#image_path = PathChooser(default_directory=imagesdir)\n",
    "#display.display(image_path)\n",
    "#fname = slices_path.chosen_path\n",
    "\n",
    "sample_image = dxchange.reader.read_tiff(fname)\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "plt.imshow(sample_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preselect the data output directory for viewing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the directory to preview (currently only one directory for data)\n",
    "slices_path = PathChooser(default_directory=sessiondir)\n",
    "slices_path.chosen_path = imagesdir\n",
    "slices_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the full image stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipysliceviewer import SliceViewer\n",
    "#s = SliceViewer(default_directory=slices_path.chosen_path)\n",
    "#display.display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if localcluster:    \n",
    "    dask_client.shutdown()\n",
    "    dask_client.close()\n",
    "else:\n",
    "    dask_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomopy_als",
   "language": "python",
   "name": "tomopy_als"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
