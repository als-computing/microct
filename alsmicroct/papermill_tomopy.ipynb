{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import dxchange\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipypathchooser import PathChooser\n",
    "from ipysliceviewer import SliceViewer\n",
    "\n",
    "from distributed_recon import ReconstructionAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to our Dask cluster running inside of a Cori job, or a local Dask cluster if no job\n",
    "from dask.distributed import Client, as_completed\n",
    "\n",
    "localcluster = False\n",
    "if os.path.exists(\"dask_client\"):\n",
    "    try:\n",
    "        with open(\"dask_client\", 'r') as f:\n",
    "            scheduler_file = f.read().strip()\n",
    "        \n",
    "        dask_client = Client(scheduler_file=scheduler_file)\n",
    "    except Exception as e:\n",
    "        print(\"Unable to use existing dask_client file to connect to a Dask cluster!\")\n",
    "        localcluster = True\n",
    "else:\n",
    "    localcluster = True\n",
    "\n",
    "if localcluster:\n",
    "    # No Dask cluster present, we will start a local cluster\n",
    "    from dask.distributed import LocalCluster    \n",
    "    \n",
    "    cluster = LocalCluster(n_workers=4, threads_per_worker=2)\n",
    "    dask_client = Client()\n",
    "    \n",
    "dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set inputs and directories for analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RTV_18A_air_760torr_08_fast\"\n",
    "\n",
    "inputdir = PathChooser()\n",
    "inputdir.chosen_path = '/global/cfs/cdirs/als/users/parkinson/SLS_Feb2019/disk1/RTV_18A_air_760torr_08_fast'\n",
    "inputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratchdir = os.path.expandvars(\"$SCRATCH\")\n",
    "input_stagedir = os.path.join(scratchdir, \"als_reconstruction_inputs\")\n",
    "\n",
    "# we will create a directory to store all of our analyses for this session\n",
    "sessiondir = os.path.join(scratchdir, \"{}_{}\".format(dataset_name, datetime.datetime.now().isoformat()))\n",
    "os.mkdir(sessiondir)\n",
    "\n",
    "# copy our input data directory to $SCRATCH for execution\n",
    "dataset_stagedir = os.path.join(input_stagedir, dataset_name)\n",
    "\n",
    "# create the input directory on $SCRATCH as needed and copy the data\n",
    "if not os.path.exists(dataset_stagedir):\n",
    "    shutil.copytree(inputdir.chosen_path, dataset_stagedir)\n",
    "    \n",
    "# this is the notebook we will be running through papermill, which calls the reconstruction code\n",
    "currentdir = os.getcwd()\n",
    "template_nb = os.path.join(currentdir, 'tomopy_recon_template.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define initial parameters and analysis inputs, then process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters we want to pass to the notebook\n",
    "generic_params = dict(\n",
    "    filename = \"RTV_18A_air_760torr_08_fast.h5\",\n",
    "    inputPath = dataset_stagedir,\n",
    "    chunk_proj = 1,\n",
    "    chunk_sino = 1,\n",
    "    ncore = 1,\n",
    "    filetype = 'sls'\n",
    ")\n",
    "\n",
    "initial_timepoints = list(range(0,10))\n",
    "\n",
    "# run with default values\n",
    "defaults_analysis = ReconstructionAnalysis(\n",
    "    template_nb=template_nb,\n",
    "    label=\"{}_defaults\".format(generic_params[\"filename\"]),\n",
    "    description=\"Processing {} with default values\".format(generic_params[\"filename\"]))\n",
    "\n",
    "completed, failed = defaults_analysis.run_analysis(\n",
    "    outputdir=sessiondir,\n",
    "    params=generic_params,\n",
    "    timepoints=initial_timepoints,\n",
    "    dask_client=dask_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a quick preview of a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesdir = os.path.join(sessiondir, defaults_analysis._outputdir, \"images\")\n",
    "last_index = initial_timepoints[-1]\n",
    "\n",
    "# preselect an image in the middle of the stack\n",
    "mid_timepoint = \"{:05d}\".format(int((last_index / 2)) + 1)\n",
    "fname = os.path.join(\n",
    "    imagesdir, \n",
    "    \"{}_{}.tiff\".format(dataset_name, mid_timepoint))\n",
    "\n",
    "# Alternatively, select an image to preview\n",
    "#image_path = PathChooser(default_directory=imagesdir)\n",
    "#display.display(image_path)\n",
    "#fname = slices_path.chosen_path\n",
    "\n",
    "sample_image = dxchange.reader.read_tiff(fname)\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "plt.imshow(sample_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the full image stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipysliceviewer import SliceViewer\n",
    "s = SliceViewer(default_directory=imagesdir)\n",
    "display.display(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the same dataset using multiple parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture all of our processing runs\n",
    "analyses = []\n",
    "\n",
    "# experiment with butterworth cutoff values\n",
    "param_set_bcutoff = [\n",
    "    dict(**generic_params, cor=385, butterworth_cutoff=n) for n in [0.05, 0.1, 0.2]]\n",
    "\n",
    "# experiment with pipeline options\n",
    "param_set_pipeline_options = [\n",
    "    dict(**generic_params, doPhaseRetrieval=True),\n",
    "    dict(**generic_params, doOutliers1D=True),\n",
    "    dict(**generic_params, doPolarRing=True),\n",
    "    dict(**generic_params, doPolarRing2=True)\n",
    "]\n",
    "\n",
    "for p in param_set_bcutoff:\n",
    "    cutoff = p['butterworth_cutoff']\n",
    "    \n",
    "    cutoff_analysis = ReconstructionAnalysis(\n",
    "        template_nb=template_nb,\n",
    "        label=\"{}_bcutoff_{}\".format(dataset_name, cutoff),\n",
    "        description=\"Processing {} with default values and a butterworth cutoff of {}\".format(\n",
    "            generic_params[\"filename\"],\n",
    "            cutoff))\n",
    "    completed, failed = cutoff_analysis.run_analysis(\n",
    "        outputdir=sessiondir,\n",
    "        params=p,\n",
    "        timepoints=initial_timepoints,\n",
    "        dask_client=dask_client)\n",
    "    \n",
    "    analyses.append(cutoff_analysis)\n",
    "    \n",
    "#for p in param_set_pipeline_options:\n",
    "#    mod = None\n",
    "#    for k in p:\n",
    "#        if k not in generic_params:\n",
    "#            mod = k\n",
    "    \n",
    "#    pipeline_options_analysis = ReconstructionAnalysis(\n",
    "#        template_nb=template_nb,\n",
    "#        label=\"{}_pipeline_option_{}\".format(generic_params[\"filename\"], mod),\n",
    "#        description=\"Processing {} with default values and a pipeline option of {}\".format(\n",
    "#            generic_params[\"filename\"],\n",
    "#            mod))\n",
    "#    completed, failed = pipeline_options_analysis.run_analysis(\n",
    "#        outputdir=sessiondir,\n",
    "#        params=p,\n",
    "#        timepoints=initial_timepoints,\n",
    "#        dask_client=dask_client)\n",
    "    \n",
    "#    analyses.append(pipeline_options_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the output for each analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpl_preview_image(analysis, timepoint=None):\n",
    "    imagesdir = os.path.join(analysis._outputdir, \"images\")\n",
    "    last_index = initial_timepoints[-1]\n",
    "\n",
    "    if timepoint is None:    \n",
    "        # preselect an image in the middle of the stack\n",
    "        timepoint = \"{:05d}\".format(int((last_index / 2)) + 1)\n",
    "\n",
    "    fname = os.path.join(\n",
    "        imagesdir, \n",
    "        \"{}_{}.tiff\".format(analysis._params['filename'].rsplit('.', 1)[0], timepoint))\n",
    "\n",
    "    sample_image = dxchange.reader.read_tiff(fname)    \n",
    "    matplotlib.rcParams['figure.dpi'] = 300\n",
    "    plt.title(analysis.label)\n",
    "    plt.imshow(sample_image, cmap=\"gray\")\n",
    "\n",
    "for recon_analysis in analyses:\n",
    "    mpl_preview_image(recon_analysis)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the Dask connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if localcluster:    \n",
    "    dask_client.shutdown()\n",
    "    dask_client.close()\n",
    "else:\n",
    "    dask_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomopy_als",
   "language": "python",
   "name": "tomopy_als"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
