{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from base64 import b64decode\n",
    "\n",
    "from IPython import display\n",
    "from tqdm.auto import tqdm\n",
    "import nbclient.exceptions\n",
    "\n",
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"papermill_tomopy_{}\"\n",
    "\n",
    "# write results to $SCRATCH\n",
    "curdir = os.getcwd()\n",
    "scratchdir = os.path.expandvars(\"$SCRATCH\")\n",
    "sessiondir = os.path.join(scratchdir, task_prefix.format(datetime.datetime.now().isoformat()))\n",
    "\n",
    "# create the output directory as needed\n",
    "if not os.path.exists(sessiondir):\n",
    "    os.mkdir(sessiondir)\n",
    "\n",
    "# copy our input data directory to $SCRATCH for execution\n",
    "src = '/global/cfs/cdirs/als/users/parkinson/SLS_Feb2019/disk1/RTV_18A_air_760torr_08_fast'\n",
    "inputdir = os.path.join(scratchdir, \"RTV_18A_air_760torr_08_fast\")\n",
    "\n",
    "# create the input directory on $SCRATCH as needed and copy the data\n",
    "if not os.path.exists(inputdir):\n",
    "    shutil.copytree(src, inputdir)\n",
    "\n",
    "# set the parameters we want to pass to the notebook\n",
    "params = dict(\n",
    "    filename = \"RTV_18A_air_760torr_08_fast.h5\",\n",
    "    inputPath = inputdir,\n",
    "    outputPath = sessiondir,\n",
    "    chunk_proj=1,\n",
    "    chunk_sino=1,\n",
    "    ncore=1,\n",
    "    filetype='sls'\n",
    ")\n",
    "\n",
    "# this is the notebook we will be running through papermill, which calls the reconstruction code\n",
    "in_nb = os.path.join(curdir, 'tomopy_recon_template.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.128.0.25:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/mhenders/cori-shared-node-cpu/proxy/8787/status' target='_blank'>/user/mhenders/cori-shared-node-cpu/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>32</li>\n",
       "  <li><b>Cores: </b>64</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.128.0.25:8786' processes=32 threads=64>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to our Dask cluster running inside of a Cori job, or a local Dask cluster if no job\n",
    "from dask.distributed import Client, as_completed\n",
    "\n",
    "localcluster = False\n",
    "if os.path.exists(\"dask_client\"):\n",
    "    try:\n",
    "        with open(\"dask_client\", 'r') as f:\n",
    "            scheduler_file = f.read().strip()\n",
    "        \n",
    "        dask_client = Client(scheduler_file=scheduler_file)\n",
    "    except Exception as e:\n",
    "        print(\"Unable to use existing dask_client file to connect to a Dask cluster!\")\n",
    "        localcluster = True\n",
    "else:\n",
    "    localcluster = True\n",
    "\n",
    "if localcluster:\n",
    "    # No Dask cluster present, we will start a local cluster\n",
    "    from dask.distributed import LocalCluster    \n",
    "    \n",
    "    cluster = LocalCluster(n_workers=4, threads_per_worker=2)\n",
    "    dask_client = Client()\n",
    "    \n",
    "dask_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af749218a75a456cade53ca406b4d211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Tasks submitted', max=10.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RTV_18A_air_760torr_08_fast.h5: 10 tasks submitted, 10 timepoints\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db11edf2ac044ee9e0f40d180de7229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Tasks completed', max=10.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process all chunks\n",
    "#timepoints range from 0 to 105\n",
    "first_timepoint = 0\n",
    "last_timepoint = 10\n",
    "submits = []\n",
    "fails = []\n",
    "\n",
    "# save the input parameters out to a file with the output notebooks and data\n",
    "input_params_file = os.path.join(sessiondir, task_prefix.format(\"input_parameters.json\"))\n",
    "with open(input_params_file, 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "# submit all papermill tasks to Dask\n",
    "num_points = last_timepoint - first_timepoint\n",
    "with tqdm(total=num_points, desc=\"Tasks submitted\", unit=\"task\") as submits_pbar:\n",
    "    for timepoint in range(first_timepoint, last_timepoint):\n",
    "        params['timepoint'] = timepoint\n",
    "        out_nb = os.path.join(sessiondir, '{}.ipynb'.format(task_prefix.format(timepoint)))\n",
    "\n",
    "        submits.append(\n",
    "            dask_client.submit(\n",
    "                pm.execute_notebook, \n",
    "                in_nb, \n",
    "                out_nb, \n",
    "                params,\n",
    "                start_timeout=60,\n",
    "                progress_bar=False))\n",
    "        submits_pbar.update(1)\n",
    "        time.sleep(0.25)\n",
    "\n",
    "print(\"{}: {} tasks submitted, {} timepoints\".format(params['filename'], len(submits), num_points))\n",
    "\n",
    "last_exc = None\n",
    "completed = []\n",
    "failed = []\n",
    "# wait for all the tasks to complete\n",
    "with tqdm(total=len(submits), desc=\"Tasks completed\", unit=\"task\") as completed_pbar:\n",
    "    for future in as_completed(submits):\n",
    "        try:\n",
    "            x = future.result()\n",
    "            completed.append(x)\n",
    "        except nbclient.exceptions.DeadKernelError as e:\n",
    "            timepoint = submits.index(future)\n",
    "            params['timepoint'] = timepoint\n",
    "            failed.append(params)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            timepoint = submits.index(future)\n",
    "            params['timepoint'] = timepoint\n",
    "            failed.append(params)\n",
    "        finally:\n",
    "            completed_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup any remaining tasks, if present\n",
    "\n",
    "resubmits = []\n",
    "while len(failed) > 0:\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    with tqdm(total=num_points, desc=\"Tasks resubmitted\", unit=\"task\") as resubmits_pbar:\n",
    "        for task_params in failed:\n",
    "            out_nb = os.path.join(sessiondir, '{}.ipynb'.format(task_prefix.format(task_params['timepoint'])))\n",
    "\n",
    "            resubmits.append(\n",
    "                dask_client.submit(pm.execute_notebook, in_nb, out_nb, task_params, progress_bar=False))\n",
    "            resubmits_pbar.update(1)\n",
    "\n",
    "    with tqdm(total=len(resubmits), desc=\"Cleanup Tasks completed\", unit=\"task\") as cleanup_pbar:\n",
    "        for future in as_completed(resubmits):\n",
    "            try:\n",
    "                x = future.result()\n",
    "                \n",
    "                # clear the failure\n",
    "                i = resubmits.index(future)\n",
    "                failed[i] = None\n",
    "                resubmits[i] = None\n",
    "                completed.append(x)\n",
    "            except RuntimeError as e:\n",
    "                last_exc = e    \n",
    "            finally:\n",
    "                cleanup_pbar.update(1)\n",
    "    \n",
    "    failed = [f for f in failed if f is not None]\n",
    "    resubmits = [r for r in resubmits if r is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the directory to preview (currently only one directory for data)\n",
    "from ipypathchooser import PathChooser\n",
    "slices_path = PathChooser(default_directory=sessiondir)\n",
    "slices_path.chosen_path = os.path.join(sessiondir, \"rec\", os.path.splitext(params['filename'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9992211815499d8ced8d583123c9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SliceViewer(children=(PathChooser(current_directory='/global/cscratch1/sd/mhenders/papermill_tomopy_2020-11-03…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import io\n",
    "#from PIL import Image\n",
    "#import numpy as np\n",
    "\n",
    "# preview the slice outputs for the current set of params\n",
    "from IPython import display\n",
    "from ipysliceviewer import SliceViewer\n",
    "\n",
    "#imagestack = []\n",
    "#for x in completed:\n",
    "#    imagestack.append(\n",
    "#        np.array(\n",
    "#            Image.open(\n",
    "#                io.BytesIO(\n",
    "#                    b64decode(x['cells'][-3]['outputs'][1]['data']['image/png'])))))\n",
    "#np_imagestack = np.array(imagestack)\n",
    "#s = SliceViewer(volume=np_imagestack)\n",
    "\n",
    "s = SliceViewer(default_directory=slices_path.chosen_path)\n",
    "display.display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if localcluster:    \n",
    "    dask_client.shutdown()\n",
    "    dask_client.close()\n",
    "else:\n",
    "    dask_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomopy_als",
   "language": "python",
   "name": "tomopy_als"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
